@article{Zhao2024,
   abstract = {Object detection in visible (RGB) and infrared (IR) images has been widely applied in recent years. Leveraging the complementary characteristics of RGB and IR images, the object detector provides reliable and robust object localization from day to night. Existing fusion strategies directly inject RGB and IR images into convolution neural networks, leading to inferior detection performance. Since the RGB and IR features have modality-specific noise, these strategies will worsen the fused features along with the propagation. Inspired by the mechanism of human brain processing multimodal information, this work introduces a new coarse-to-fine perspective to purify and fuse two modality features. Specifically, following this perspective, we design a Redundant Spectrum Removal module to coarsely remove interfering information within each modality and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called Removal and Selection Detector (RSDet). Extensive experiments on three RGB-IR object detection datasets verify the superior performance of our method.},
   author = {Tianyi Zhao and Maoxun Yuan and Xingxing Wei},
   month = {1},
   title = {Removal and Selection: Improving RGB-Infrared Object Detection via Coarse-to-Fine Fusion},
   url = {http://arxiv.org/abs/2401.10731},
   year = {2024},
}
@article{Tateno2017,
   abstract = {Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.},
   author = {Keisuke Tateno and Federico Tombari and Iro Laina and Nassir Navab},
   month = {4},
   title = {CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction},
   url = {http://arxiv.org/abs/1704.03489},
   year = {2017},
}
@article{Yang2024,
   abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.},
   author = {Lihe Yang and Bingyi Kang and Zilong Huang and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},
   month = {1},
   title = {Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data},
   url = {http://arxiv.org/abs/2401.10891},
   year = {2024},
}
@inproceedings{Bodensteiner2012,
   abstract = {A well known problem in photogrammetry and computer vision is the precise and robust determination of camera poses with respect to a given 3D model. In this work we propose a novel multi-modal method for single image camera pose estimation with respect to 3D models with intensity information (e.g., LiDAR data with reflectance information). We utilize a direct point based rendering approach to generate synthetic 2D views from 3D datasets in order to bridge the dimensionality gap. The proposed method then establishes 2D/2D point and local region correspondences based on a novel self-similarity distance measure. Correct correspondences are robustly identified by searching for small regions with a similar geometric relationship of local self-similarities using a Generalized Hough Transform. After backprojection of the generated features into 3D a standard Perspective-n-Points problem is solved to yield an initial camera pose. The pose is then accurately refined using an intensity based 2D/3D registration approach. An evaluation on Vis/IR 2D and airborne and terrestrial 3D datasets shows that the proposed method is applicable to a wide range of different sensor types. In addition, the approach outperforms standard global multi-modal 2D/3D registration approaches based on Mutual Information with respect to robustness and speed. Potential applications are widespread and include for instance multi-spectral texturing of 3D models, SLAM applications, sensor data fusion and multi-spectral camera calibration and super-resolution applications. Â© 2012 Springer-Verlag.},
   author = {Christoph Bodensteiner and Marcus Hebel and Michael Arens},
   doi = {10.1007/978-3-642-35740-4_23},
   isbn = {9783642357398},
   issn = {03029743},
   issue = {PART 2},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Multi-Modal 2D/3D Correspondences,Multi-Modal Registration,Pose Estimation,Self-Similarity Distance Measure},
   pages = {296-309},
   title = {Accurate single image multi-modal camera pose estimation},
   volume = {6554 LNCS},
   url = {https://www.researchgate.net/publication/224903558_Accurate_Single_Image_Multi-Modal_Camera_Pose_Estimation},
   year = {2012},
}
@book{Yuan2013,
   abstract = {Inhaltl. Gliederung in Vol. 1 - 3 Jointly held with the 6th International Conference on Biomedical Engineering and Informatics (BMEI 2013) V. 1. pages 1-587 -- v. 2. pages 588-1178 -- v. 3. pages 1179- 1753},
   author = {Zhenming Yuan and Hangzhou Normal University and IEEE Engineering in Medicine and Biology Society and IEEE International Conference on Service-Oriented Computing and Applications and International Congress on Image and Signal Processing 6 2013.12.16-18 Hangzhou and CISP 6 2013.12.16-18 Hangzhou},
   isbn = {9781479927647},
   title = {Registration of Infrared and Visible Images Based on
the Correlation of the Edges},
   url = {https://ieeexplore.ieee.org/document/6745309},
   year = {2013},
}
@article{Du2018,
   abstract = {Registration of multi-sensor data is a prerequisite for multimodal image analysis such as image fusion. This paper focuses on the problem of infrared and visible image registration, which has played an important role for the purpose of enhancing visual perception. Existing methods based on multimodal feature descriptor such as partial intensity invariant feature descriptor (PIIFD) usually fail in correctly aligning infrared and visible image pairs, due to their significant differences in resolution and appearance. In this paper, we propose a scale-invariant PIIFD (SI-PIIFD) feature and a robust feature matching method to address this problem. Specifically, we first extract corner points as control point candidates since they are usually sufficient and uniformly distributed across the image domain. Then, the SI-PIIFDs are calculated for all corner points and matched according to the descriptor similarity together with a locality preserving geometric constraint. Subsequently, we model the spatial transformation between an infrared and visible image pair with an affine function and introduce a robust Bayesian framework to estimate it from the SI-PIIFD feature matches even if they contaminated by false matches. Finally, the backward approach is chosen for image transformation to avoid holes and overlaps in the output image. Extensive experiments on a challenging dataset with comparisons to other state-of-the-arts demonstrate the effectiveness of the proposed method, both in terms of accuracy and efficiency.},
   author = {Qinglei Du and Aoxiang Fan and Yong Ma and Fan Fan and Jun Huang and Xiaoguang Mei},
   doi = {10.1109/ACCESS.2018.2877642},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Infrared,feature matching,image registration,multimodal descriptor,robust estimation},
   pages = {64107-64121},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Infrared and Visible Image Registration Based on Scale-Invariant PIIFD Feature and Locality Preserving Matching},
   volume = {6},
   url = {https://ieeexplore.ieee.org/document/8502747},
   year = {2018},
}
@article{Ji2022,
   abstract = {Image registration is the base of subsequent image processing and has been widely utilized in computer vision. Aiming at the differences in the resolution, spectrum, and viewpoint of infrared and visible images, and in order to accurately register infrared and visible images, an automatic robust infrared and visible image registration algorithm, based on a deep convolutional network, was proposed. In order to precisely search and locate the feature points, a deep convolutional network is introduced, which solves the problem that a large number of feature points can still be extracted when the pixels of the infrared image are not clear. Then, in order to achieve accurate feature point matching, a rough-to-fine matching algorithm is designed. The rough matching is obtained by location orientation scale transform Euclidean distance, and then, the fine matching is performed based on the update global optimization, and finally, the image registration is realized. Experimental results show that the proposed algorithm has better robustness and accuracy than several advanced registration algorithms.},
   author = {Jingyu Ji and Yuhua Zhang and Zhilong Lin and Yongke Li and Changlong Wang and Yongjiang Hu and Jiangyi Yao},
   doi = {10.3390/electronics11111674},
   issn = {20799292},
   issue = {11},
   journal = {Electronics (Switzerland)},
   keywords = {deep convolutional network,image extraction,image matching,image registration,infrared and visible image},
   month = {6},
   publisher = {MDPI},
   title = {Infrared and Visible Image Registration Based on Automatic Robust Algorithm},
   volume = {11},
   url = {https://www.mdpi.com/2079-9292/11/11/1674},
   year = {2022},
}
@misc{Xu2014,
   abstract = {Stereo matching is a fundamental building block for many vision and robotics applications. An informative and concise cost volume representation is vital for stereo matching of high accuracy and efficiency. In this paper, we present a novel cost volume construction method which generates attention weights from correlation clues to suppress redundant information and enhance matching-related information in the concatenation volume. To generate reliable attention weights, we propose multi-level adaptive patch matching to improve the distinctiveness of the matching cost at different disparities even for textureless regions. The proposed cost volume is named attention concatena-tion volume (ACV) which can be seamlessly embedded into most stereo matching networks, the resulting networks can use a more lightweight aggregation network and meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the aggregation network can achieve higher accuracy for GwcNet. Furthermore, we design a highly accurate network (ACVNet) based on our ACV, which achieves state-of-the-art performance on several benchmarks. The code is available at https://github.com/gangweiX/ACVNet.},
   author = {Gangwei Xu and Junda Cheng and Peng Guo and Xin Yang},
   title = {Attention Concatenation Volume for Accurate and Efficient Stereo Matching},
   url = {https://github.com/gangweiX/ACVNet. https://arxiv.org/abs/2203.02146},
   year = {2014},
}
@article{Ma2016,
   abstract = {This paper presents a segmentation-based stereo matching algorithm using an adaptive multi-cost approach, which is exploited for obtaining accuracy disparity maps. The main contribution is to integrate the appealing properties of multi-cost approach into the segmentation-based framework. Firstly, the reference image is segmented by using the mean-shift algorithm. Secondly, the initial disparity of each segment is estimated by an adaptive multi-cost method, which consists of a novel multi-cost function and an adaptive support window cost aggregation strategy. The multi-cost function increases the robustness of the initial raw matching costs calculation and the adaptive window reduces the matching ambiguity effectively. Thirdly, an iterative outlier suppression and disparity plane parameters fitting algorithm is designed to estimate the disparity plane parameters. Lastly, an energy function is formulated in segment domain, and the optimal plane label is approximated by belief propagation. The experimental results with the Middlebury stereo datasets, along with synthesized and real-world stereo images, demonstrate the effectiveness of the proposed approach.},
   author = {Ning Ma and Yubo Men and Chaoguang Men and Xiang Li},
   doi = {10.3390/sym8120159},
   issn = {20738994},
   issue = {12},
   journal = {Symmetry},
   keywords = {Belief propagation,Disparity plane fitting,Image segmentation,Multi-cost,Stereo matching},
   publisher = {MDPI AG},
   title = {Accurate dense stereo matching based on image segmentation using an adaptive multi-cost approach},
   volume = {8},
   url = {https://www.mdpi.com/2073-8994/8/12/159},
   year = {2016},
}
@misc{Wang2003,
   abstract = {The structural similarity image quality paradigm is based on the assumption that the human visual system is highly adapted for extracting structural information from the scene, and therefore a measure of structural similarity can provide a good approximation to perceived image quality. This paper proposes a multi-scale structural similarity method, which supplies more flexibility than previous single-scale methods in incorporating the variations of viewing conditions. We develop an image synthesis method to calibrate the parameters that define the relative importance of different scales. Experimental comparisons demonstrate the effectiveness of the proposed method.},
   author = {Zhou Wang and Eero P Simoncelli and Alan C Bovik},
   title = {MULTI-SCALE STRUCTURAL SIMILARITY FOR IMAGE QUALITY ASSESSMENT},
   url = {https://ieeexplore.ieee.org/document/1292216},
   year = {2003},
}
@book{Chengtao2024,
   abstract = {"IEEE Catalog Number:"CFP18839-CDR."},
   author = {Chengtao Cai and Xin Ding and IEEE Robotics and Automation Society. and Institute of Electrical and Electronics Engineers},
   isbn = {9781538660751},
   title = {Registration of Infrared and Visible Image
Based on OpenCV},
   url = {https://ieeexplore.ieee.org/document/8484591},
   year = {2024},
}
@article{Liu2018,
   abstract = {Visual saliency is a type of visual feature which simulates visual attention selection mechanism in biological system and has better robustness and invariance. A rapid infrared image and visible light image registration method based on visual saliency and SIFT (scale invariant feature transform) is proposed in this paper. The method adopts amplitude modulation Fourier transform to construct saliency map, and the image salient points are achieved preliminarily by salient threshold. Subsequently, this method calculates the local entropy for these salient points because of entropyâs character for information measurement, then these points are reordered and screened based on the strategy of entropy priority. The screened results are thought as centers for salient regions. Morphological operation is used for growing and merging for neighbor salient scene region in image. Aiming at the abstracted salient region for image, PCA (principal component analysis)-SIFT algorithm is proposed, which can produce the compressed SIFT registration features and largely reduce the computational cost of image registration. The proposed algorithm adopts random sampling conformance method to remove the mistaken point pairs before calculating the model parameter of affine transformation for registration between infrared image and visible lights. The experimental results indicate that the method has good invariance under image scale, rotation, translation, and illumination variation and can realize effective registration between infrared image and visible lights. Compared with some classical algorithms, the proposed method has advantage in registration accuracy and registration speed obviously.},
   author = {Gang Liu and Zhonghua Liu and Sen Liu and Jianwei Ma and Fei Wang},
   doi = {10.1186/s13640-018-0283-9},
   issn = {16875281},
   issue = {1},
   journal = {Eurasip Journal on Image and Video Processing},
   keywords = {Image registration,Infrared and visible light,Principal component analysis,Scale invariant feature transform,Visual saliency},
   month = {12},
   publisher = {Springer International Publishing},
   title = {Registration of infrared and visible light image based on visual saliency and scale invariant feature transform},
   volume = {2018},
   url = {https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0283-9},
   year = {2018},
}
@article{Ozer2024,
   abstract = {This paper proposes a deep learning based solution for multi-modal image alignment regarding UAV-taken images. Many recently proposed state-of-the-art alignment techniques rely on using Lucas-Kanade (LK) based solutions for a successful alignment. However, we show that we can achieve state of the art results without using LK-based methods. Our approach carefully utilizes a two-branch based convolutional neural network (CNN) based on feature embedding blocks. We propose two variants of our approach, where in the first variant (ModelA), we directly predict the new coordinates of only the four corners of the image to be aligned; and in the second one (ModelB), we predict the homography matrix directly. Applying alignment on the image corners forces algorithm to match only those four corners as opposed to computing and matching many (key)points, since the latter may cause many outliers, yielding less accurate alignment. We test our proposed approach on four aerial datasets and obtain state of the art results, when compared to the existing recent deep LK-based architectures.},
   author = {Sedat Ozer and Alain P. Ndigande},
   month = {2},
   title = {VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs},
   url = {http://arxiv.org/abs/2402.09635},
   year = {2024},
}
@article{Jiang2023,
   abstract = {Since the differences in viewing range, resolution and relative position, the multi-modality sensing module composed of infrared and visible cameras needs to be registered so as to have more accurate scene perception. In practice, manual calibration-based registration is the most widely used process, and it is regularly calibrated to maintain accuracy, which is time-consuming and labor-intensive. To cope with these problems, we propose a scene-adaptive infrared and visible image registration. Specifically, in regard of the discrepancy between multi-modality images, an invertible translation process is developed to establish a modality-invariant domain, which comprehensively embraces the feature intensity and distribution of both infrared and visible modalities. We employ homography to simulate the deformation between different planes and develop a hierarchical framework to rectify the deformation inferred from the proposed latent representation in a coarse-to-fine manner. For that, the advanced perception ability coupled with the residual estimation conducive to the regression of sparse offsets, and the alternate correlation search facilitates a more accurate correspondence matching. Moreover, we propose the first ground truth available misaligned infrared and visible image dataset, involving three synthetic sets and one real-world set. Extensive experiments validate the effectiveness of the proposed method against the state-of-the-arts, advancing the subsequent applications.},
   author = {Zhiying Jiang and Zengxi Zhang and Jinyuan Liu and Xin Fan and Risheng Liu},
   month = {4},
   title = {Breaking Modality Disparity: Harmonized Representation for Infrared and Visible Image Registration},
   url = {http://arxiv.org/abs/2304.05646},
   year = {2023},
}
